{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-4927  GCE       2                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: gensim in /opt/conda/miniconda3/lib/python3.10/site-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy>=1.7.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from gensim) (1.9.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/miniconda3/lib/python3.10/site-packages (from gensim) (7.0.1)\n","Requirement already satisfied: wrapt in /opt/conda/miniconda3/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install gensim"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import threading\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import math\n","import gzip\n","import csv\n","import io\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  9 17:39 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-4927-m.c.ir-ex-3-mapreduce.internal:33275\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fde3c513820>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'ruby_bucket_327064358' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and b.name.startswith(\"multi\"):\n","        paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":8,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":9,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":10,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex, MultiFileWriter, MultiFileReader"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"02f81c72"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":11,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n"]},{"cell_type":"code","execution_count":12,"id":"e14c3313","metadata":{},"outputs":[],"source":["#doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"f6375562"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":13,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[],"source":["# Count number of wiki pages\n","# parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":14,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["stemmer = PorterStemmer()\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id, stem = False):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  if stem is True:\n","      tokens = [stemmer.stem(token) for token in tokens]\n","  # YOUR CODE HERE\n","  dic = {}\n","  for token in tokens:\n","    if token not in all_stopwords:\n","      if token not in dic:\n","        dic[token] = 1\n","      else:\n","        dic[token] += 1\n","\n","  def func(x):\n","    return (x, (id,dic[x]))\n","\n","  return list(map(func, dic.keys()))\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  result = defaultdict(int)\n","\n","  # Iterate over the list and accumulate the second values\n","  for key, value in unsorted_pl:\n","      result[key] += value\n","\n","  # Convert the defaultdict back to a list of tuples\n","  result_list = list(result.items())\n","\n","  return sorted(result_list, key = lambda x: x[0])\n","\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  return postings.map(lambda x: (x[0], len(x[1])))\n","\n","def partition_postings_and_write(base_dir, postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  buckets = postings.groupBy(lambda x: token2bucket_id(x[0]))\n","  obj = InvertedIndex()\n","  ret = buckets.map(lambda x: obj.write_a_posting_list(x, base_dir, bucket_name = 'ruby_bucket_327064358' ))\n","  return ret\n"]},{"cell_type":"markdown","id":"4fece710","metadata":{},"source":["# building an inverted index for the titles, with stemming"]},{"cell_type":"code","execution_count":100,"id":"63916b71","metadata":{"id":"72bcf46a"},"outputs":[],"source":["doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"code","execution_count":101,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1], True))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(base_dir = 'title_index', postings = postings_filtered).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":103,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title_index'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"VhAV0A6dNZWY"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":104,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 62.6 MiB/ 62.6 MiB]                                                \n","Operation completed over 1 objects/62.6 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'title_index')\n","# upload to gs\n","index_src = \"title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":105,"id":"8f880d59","metadata":{"id":"msogGbJ3c8JF","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["  62.6 MiB  2024-03-05T14:07:04Z  gs://ruby_bucket_327064358/title_index/title_index.pkl\r\n","TOTAL: 1 objects, 65640595 bytes (62.6 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"1a2f3962","metadata":{},"source":["# building an inverted index for the titles, without stemming"]},{"cell_type":"code","execution_count":71,"id":"0afb86d0","metadata":{},"outputs":[],"source":["doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"code","execution_count":72,"id":"46c085c5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(base_dir = 'title_index_no_stem', postings = postings_filtered).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":73,"id":"734a6907","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title_index_no_stem'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":74,"id":"439a6388","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index_no_stem.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 67.6 MiB/ 67.6 MiB]                                                \n","Operation completed over 1 objects/67.6 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'title_index_no_stem')\n","# upload to gs\n","index_src = \"title_index_no_stem.pkl\"\n","index_dst = f'gs://{bucket_name}/title_index_no_stem/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":75,"id":"09b562b9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  67.6 MiB  2024-03-05T15:37:46Z  gs://ruby_bucket_327064358/title_index_no_stem/title_index_no_stem.pkl\r\n","TOTAL: 1 objects, 70887731 bytes (67.6 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"cb530ce2","metadata":{},"source":["# building an inverted index for the body text, with stemming"]},{"cell_type":"code","execution_count":16,"id":"09c5c9ce","metadata":{},"outputs":[],"source":["doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n"]},{"cell_type":"code","execution_count":17,"id":"a246e262","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1], True))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(base_dir = 'body_index_with_stem', postings = postings_filtered).collect()\n","index_const_time = time() - t_start\n"]},{"cell_type":"code","execution_count":18,"id":"162b3591","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='body_index_with_stem'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":19,"id":"1690f343","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://body_index_with_stem.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 15.9 MiB/ 15.9 MiB]                                                \n","Operation completed over 1 objects/15.9 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'body_index_with_stem')\n","# upload to gs\n","index_src = \"body_index_with_stem.pkl\"\n","index_dst = f'gs://{bucket_name}/body_index_with_stem/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":20,"id":"6323da31","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 15.94 MiB  2024-03-06T03:29:02Z  gs://ruby_bucket_327064358/body_index_with_stem/body_index_with_stem.pkl\r\n","TOTAL: 1 objects, 16715670 bytes (15.94 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"fc0667a9","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# PageRank"]},{"cell_type":"markdown","id":"0875c6bd","metadata":{"id":"fdd1bdca","nbgrader":{"grade":false,"grade_id":"cell-2fee4bc8d83c1e2a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["import the pagerank matrix from the bucket"]},{"cell_type":"code","execution_count":13,"id":"31a516e2","metadata":{"id":"yVjnTvQsegc-"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/05 11:26:29 WARN TaskSetManager: Stage 2 contains a task of very large size (110953 KiB). The maximum recommended task size is 1000 KiB.\n","                                                                                \r"]}],"source":["\n","# bucket_name = 'ruby_bucket_327064358'\n","# storage_client = storage.Client()\n","# bucket = storage_client.bucket(bucket_name)\n","# file_path = 'pr/part-00000-0412bc70-0308-4be4-9402-8e8e513b3def-c000.csv.gz'\n","# blob = bucket.blob(file_path)\n","# blob_data = blob.download_as_string()\n","\n","# # Decompress the gzip data\n","# decompressed_data = gzip.decompress(blob_data)\n","\n","# # Decode the decompressed data as text\n","# decompressed_text = decompressed_data.decode('utf-8')\n","\n","# # Create a StringIO object to treat the text as a file-like object\n","# csv_data = io.StringIO(decompressed_text)\n","\n","# # Read the CSV data\n","# csv_reader = csv.reader(csv_data)\n","\n","# page_rank_rdd = sc.parallelize(list(csv_reader))\n","\n","# page_rank_rdd = page_rank_rdd.map(lambda x: (int(x[0]), float(x[1])))\n","                                  \n","\n","# # Convert RDD to dictionary (map)\n","# page_rank_map = page_rank_rdd.collectAsMap()\n"]},{"cell_type":"code","execution_count":14,"id":"635578a0","metadata":{},"outputs":[],"source":["# blob = bucket.blob('global_dics/page_rank.pkl')\n","# blob.upload_from_string(pickle.dumps(page_rank_map))"]},{"cell_type":"markdown","id":"c561b830","metadata":{},"source":["## word2vec training"]},{"cell_type":"code","execution_count":17,"id":"086c5d18","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":13,"id":"dbdc1c18","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["titles = parquetFile.select(\"title\").rdd\n","titles = titles.map(lambda x: x.title)\n","titles = titles.collect()\n"]},{"cell_type":"code","execution_count":15,"id":"4ccf5069","metadata":{},"outputs":[{"data":{"text/plain":["['Foster Air Force Base',\n"," 'Torino Palavela',\n"," 'Mad About the Boy',\n"," 'Shayne Breuer',\n"," 'Parantaka I',\n"," 'Arundel (UK Parliament constituency)',\n"," 'Andrew Martinez',\n"," 'Vancouver VooDoo',\n"," 'Invisible plane',\n"," 'Shopping channel']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["titles[:10]"]},{"cell_type":"code","execution_count":24,"id":"932fce43","metadata":{},"outputs":[],"source":["from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec"]},{"cell_type":"code","execution_count":25,"id":"93e2869d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_9941/2240112156.py:20: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n","  _ = glove2word2vec(glove_file, tmp_file)\n"]}],"source":["\n","# Initialize a GCS client\n","client = storage.Client()\n","\n","# Define the GCS bucket and file path\n","bucket_name = \"ruby_bucket_327064358\"\n","file_name = \"glove.6B.100d.txt\"\n","\n","# Define the local file path where you want to download the file\n","local_glove_file_path = \"local_glove_model.txt\"\n","local_word2vec_file_path = \"local_word2vec_model.txt\"\n","\n","# Download the GloVe model file from GCS bucket\n","bucket = client.bucket(bucket_name)\n","blob = bucket.blob(file_name)\n","blob.download_to_filename(local_glove_file_path)\n","\n","# Convert the GloVe model file to Word2Vec format\n","glove_file = local_glove_file_path\n","tmp_file = local_word2vec_file_path\n","_ = glove2word2vec(glove_file, tmp_file)\n","\n","# Load the model using the Gensim API's load function\n","model = KeyedVectors.load_word2vec_format(tmp_file)"]},{"cell_type":"code","execution_count":33,"id":"78f0150d","metadata":{},"outputs":[{"data":{"text/plain":["['writer', 'novelist']"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["closest = model.most_similar(positive=['author'], topn=10)\n","closest = list(map(lambda x: x[0], closest))\n","\n","closest[:2]"]},{"cell_type":"code","execution_count":null,"id":"73bb7b69","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}