
### with word2vec
#
# def search_query_title_bm25(query):
#   stemmer = PorterStemmer()
#   query_weights = {}
#   doc_scores = defaultdict(float)
#   query = tokenize(query.lower())
#
#   # newq = query[:]
#   # for word in query:
#   #   try:
#   #     closest = model.most_similar(positive=[word], topn=2)
#   #     for synonym in closest:
#   #       newq.append(synonym[0])
#   #   except Exception:
#   #     continue
#
#   newq = [stemmer.stem(token) for token in query]
#   query_length = len(newq)
#
#   # calculate weights for query terms
#   for word in newq:
#
#     # if word is not in index, pay no attention to it
#     if title_index.df.get(word, None) == None:
#       continue
#     else:
#       if query_length == 1:
#         tf = newq.count(word)  # / query_length
#         query_weights[word] = tf
#       elif idf_scores_title[word] >= 2:
#         tf = newq.count(word)  # / query_length
#         query_weights[word] = tf
#
#   if len(query_weights.items()) == 0:
#     ######## find way to calculate if word is not in index
#     return []
#
#   k1 = k3 = 1.2
#   b = 0.75
#   avg_doc_length = 2.6211557574449786  ###### avg doc length
#   docid_B = defaultdict(float)
#
#   #   for word in query_weights.keys():
#   def process_word(word):
#     pl = title_index.read_a_posting_list(base_dir='', w=word, bucket_name='ruby_bucket_327064358')
#     tf_q = query_weights[word]
#
#     for docid, tf in pl:
#       if docid in docid_B:
#         B = docid_B[docid]
#       else:
#         B = (1 - b + b * (title_lengths[docid] / avg_doc_length))
#       tf_doc = tf / title_lengths[docid]
#       with doc_locks[docid]:
#         doc_scores[docid] += idf_scores_title[word] * (((k1 + 1) * tf_doc) / (tf_doc + B * k1)) * (
#                 ((k3 + 1) * tf_q) / (k3 + tf_q))
#
#   with concurrent.futures.ThreadPoolExecutor() as executor:
#     # Submit tasks for each word in query_weights.keys()
#     for word in query_weights.keys():
#       executor.submit(process_word, word)
#
#   ret = sorted([(doc_id, score) for doc_id, score in doc_scores.items()], key=lambda x: x[1], reverse=True)[:100]
#   #ret = sorted(list(map(lambda x: (x[0], x[1] + 2 * math.log(page_rank[x[0]], 10)), ret)), key=lambda x: x[1],reverse=True)
#   # ret = list(map(lambda x: (x[0], doc_title_pairs[x[0]]), ret))
#   return ret
#
#
# def search_query_body_bm25(query):
#   # stemmer = PorterStemmer()
#   query_weights = {}
#   doc_scores = defaultdict(float)
#   query = tokenize(query.lower())
#
#   query = list(filter(lambda x: x in body_index.df, query))
#   newq = query[:]
#   if len(query) == 1:
#     # If query has only one word, get the most similar words to that word
#     try:
#       closest = model.most_similar(positive=query, topn=5)
#       for synonym in closest:
#         newq.append(synonym[0])
#     except Exception:
#       pass
#   else:
#     # If query has more than one word, calculate closest for every pair of consecutive words
#     for i in range(len(query) - 1):
#       pair = query[i:i + 2]  # Get consecutive pair of words
#       try:
#         closest = model.most_similar(positive=pair, topn=3)
#         for synonym in closest:
#           newq.append(synonym[0])
#       except Exception:
#         pass
#
#
#   # newq = query[:]
#   # for word in query:
#   #   try:
#   #     closest = model.most_similar(positive=[word], topn=3)
#   #     for synonym in closest:
#   #       newq.append(synonym[0])
#   #   except Exception:
#   #     continue
#
#   # newq = [stemmer.stem(token) for token in newq]
#   query_length = len(newq)
#
#   # calculate weights for query terms
#   for word in newq:
#     # if word is not in index, pay no attention to it
#     if body_index.df.get(word, None) == None:
#       continue
#     else:
#       if query_length == 1:
#         tf = newq.count(word)  # / query_length
#         query_weights[word] = tf
#       elif idf_scores_body[word] >= 1.5:
#         tf = newq.count(word)  # / query_length
#         query_weights[word] = tf
#
#     # return query_weights.items()
#   if len(query_weights.items()) == 0:
#     ######## find way to calculate if word is not in index
#     return []
#   k1 = k3 = 1.2
#   b = 0.75
#   avg_doc_length = 431.1623426698441  ###### avg doc length
#   docid_B = defaultdict(float)
#
#   #   for word in query_weights.keys():
#   def process_word(word):
#     pl = body_index.read_a_posting_list(base_dir='postings_gcp', w=word, bucket_name='ruby_bucket_327064358')
#     tf_q = query_weights[word]
#
#     for docid, tf in pl:
#       if docid in docid_B:
#         B = docid_B[docid]
#       else:
#         B = (1 - b + b * (doc_lengths[docid] / avg_doc_length))
#       tf_doc = tf  # / doc_lengths[docid]
#       with doc_locks[docid]:
#         doc_scores[docid] += idf_scores_body[word] * (((k1 + 1) * tf_doc) / (tf_doc + B * k1)) * (
#                 ((k3 + 1) * tf_q) / (k3 + tf_q))
#
#   with concurrent.futures.ThreadPoolExecutor() as executor:
#     # Submit tasks for each word in query_weights.keys()
#     for word in query_weights.keys():
#       executor.submit(process_word, word)
#
#   ret = sorted([(doc_id, score) for doc_id, score in doc_scores.items()], key=lambda x: x[1], reverse=True)[:100]
#   #ret = sorted(list(map(lambda x: (x[0], x[1] + 2 * math.log(page_rank[x[0]], 10)), ret)), key=lambda x: x[1],reverse=True)
#   # ret = list(map(lambda x: (x[0], doc_title_pairs[x[0]]), ret))
#   return ret
